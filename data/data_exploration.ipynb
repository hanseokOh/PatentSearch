{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import openpyxl \n",
    "\n",
    "file_path1 = '특허Text_20230724_1.xlsx'\n",
    "file_path2 = '특허Text_20230724_2.xlsx'\n",
    "sheetList=[]\n",
    "\n",
    "wb1 = openpyxl.load_workbook(file_path1)\n",
    "wb2 = openpyxl.load_workbook(file_path2)\n",
    "for i in wb1.get_sheet_names():\n",
    "    sheetList.append(i)\n",
    "\n",
    "for i in wb2.get_sheet_names():\n",
    "    sheetList.append(i)\n",
    "\n",
    "data1 = pd.ExcelFile(file_path1)\n",
    "data2 = pd.ExcelFile(file_path2)\n",
    "\n",
    "# for i in sheetList:\n",
    "#     df = pd.read_excel(data,i)\n",
    "\n",
    "df1 = pd.read_excel(data1,sheetList[0])\n",
    "df2 = pd.read_excel(data2,sheetList[1])\n",
    "\n",
    "df = df1.append(df2,sort=True,ignore_index=True)\n",
    "\n",
    "len(df), df.columns\n",
    "# Index(['특허/실용 구분', '발명의 명칭', '요약', '대표청구항', '출원번호', '출원일', '공개번호', '공개일',\n",
    "#        '등록번호', '등록일', '출원인', '발명자', '우선권 번호', '우선권 국가', '우선권 주장일',\n",
    "#        'Original IPC All'],\n",
    "#       dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_data=[]\n",
    "for id in range(len(df)):\n",
    "    instance= df.iloc[id,:].to_dict()\n",
    "    qrels_data.append({'_id':id, 'summary':instance['요약'], 'represent':instance['대표청구항'], 'title': instance['발명의 명칭'],'metadata':{'출원번호': instance['출원번호']}})\n",
    "\n",
    "len(qrels_data),qrels_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "\n",
    "q2p = defaultdict(list)\n",
    "for pair in qrels_data:\n",
    "    q2p[pair['title']].append(pair['summary'])\n",
    "\n",
    "len(q2p), pd.DataFrame([ len(v) for k,v in q2p.items()]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(len(df)):\n",
    "    instance= df.iloc[id,:].to_dict()\n",
    "    if instance['발명의 명칭']=='비콘 신호를 이용하여 도어 출입을 관리하기 위한 방법 및 시스템':\n",
    "        print(instance['출원번호'])\n",
    "        print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "for i in random.sample(qrels_data,10):\n",
    "    print(\"### 발명의 명칭:\\n\",i['title'])\n",
    "    print(\"### 요약문:\\n\",i['summary'])\n",
    "    print(\"### 대표청구항:\\n\",i['represent'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Set Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_corpus = []\n",
    "for id in range(len(df)):\n",
    "    instance= df.iloc[id,:].to_dict()\n",
    "    summary_corpus.append({'_id':id, 'text':instance['요약'], 'title': instance['발명의 명칭'],'metadata':{'type':'요약문','출원번호': instance['출원번호']}})\n",
    "    \n",
    "len(summary_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "\n",
    "all_q2id={}\n",
    "all_query_dict = {}\n",
    "\n",
    "summary_p2id={}\n",
    "\n",
    "summary_corpus_dict = {}\n",
    "summary_qrels = defaultdict(list)\n",
    "\n",
    "    \n",
    "for pair in tqdm(summary_corpus):\n",
    "    query = pair['title']\n",
    "    target = pair['text'] if type(pair['text'])!=float else '.'\n",
    "#     target = pair.get('text','None') # to remove nan\n",
    "    \n",
    "    if query in all_q2id:\n",
    "        qid = all_q2id[query]\n",
    "    else:\n",
    "        qid = f\"Q{len(all_q2id)}\"\n",
    "        all_q2id[query]=qid\n",
    "        all_query_dict[qid]=query \n",
    "\n",
    "    if target in summary_p2id:\n",
    "        pid = summary_p2id[target]\n",
    "    else:\n",
    "        pid = f\"C{len(summary_p2id)}\"\n",
    "        summary_p2id[target]=pid\n",
    "        summary_corpus_dict[pid]={'_id':pid, 'text':target, 'title': query,'metadata':{'type':'요약문','출원번호':pair['metadata']['출원번호']}}\n",
    "        \n",
    "\n",
    "    summary_qrels[qid].append(pid)\n",
    "\n",
    "len(all_q2id),len(summary_p2id),len(summary_corpus_dict),len(all_query_dict),len(summary_qrels) # (9357, 9524, 9524, 9357, 9357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "\n",
    "os.makedirs('new_summary_origin_20000',exist_ok=True)\n",
    "\n",
    "with open('new_summary_origin_20000/corpus.jsonl','w',encoding='utf-8') as f:\n",
    "    for pair in list(summary_corpus_dict.values()):\n",
    "        f.write(json.dumps(pair,indent=4,ensure_ascii=False)+'\\n')\n",
    "\n",
    "# with open('summary_corpus.txt','w',encoding='utf-8') as f:\n",
    "#     for k,v in summary_corpus_dict.items():\n",
    "#         f.write(v['text']+'\\n' if type(v['text'])!=float else '.\\n')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(summary_corpus_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries\n",
    "import json \n",
    "with open('new_summary_origin_20000/queries.jsonl','w',encoding='utf-8') as f:\n",
    "    for k,pair in all_query_dict.items():\n",
    "        f.write(json.dumps({'_id':k, 'text':pair},ensure_ascii=False)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_summary_origin_20000/summary_corpus.tsv','w',encoding='utf-8') as f:\n",
    "    f.write(f\"id\\ttitle\\ttext\\n\")\n",
    "    for k,pair in summary_corpus_dict.items():\n",
    "        f.write(f\"{pair['_id']}\\t{pair['text']}\\t{pair['title']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict \n",
    "\n",
    "train_qrels = defaultdict(list)\n",
    "dev_qrels = defaultdict(list)\n",
    "test_qrels = defaultdict(list)\n",
    "\n",
    "summary_all_pairs = [(qid,pid) for qid, pid_list in summary_qrels.items() for pid in pid_list]\n",
    "\n",
    "others, test = train_test_split(list(summary_all_pairs), test_size=0.3) # 0.7 / 0.3\n",
    "train, dev = train_test_split(list(others), test_size=0.2) # 0.7 / 0.3\n",
    "\n",
    "print(len(summary_all_pairs), len(train),len(dev),len(test)) # 20000 11200 2800 6000\n",
    "\n",
    "for pair in train: \n",
    "    qid, pid = pair\n",
    "    train_qrels[qid].append(pid)\n",
    "\n",
    "for pair in dev: \n",
    "    qid, pid = pair\n",
    "    dev_qrels[qid].append(pid)\n",
    "\n",
    "for pair in test: \n",
    "    qid, pid = pair\n",
    "    test_qrels[qid].append(pid)\n",
    "\n",
    "print(\"qrels:\", len(train_qrels), len(dev_qrels),len(test_qrels)) # qrels: 10574 2728 5784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('new_summary_origin_20000/qrels',exist_ok=True)\n",
    "\n",
    "\n",
    "###############\n",
    "#### Qrels for train, dev, test\n",
    "###############\n",
    "with open('new_summary_origin_20000/qrels/train.tsv','w', encoding='utf-8') as f:\n",
    "    f.write(f\"qid\\tpid\\score\\n\")\n",
    "    for qid,pid_list in train_qrels.items():\n",
    "        for pid in pid_list:\n",
    "            f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "with open('new_summary_origin_20000/qrels/dev.tsv','w', encoding='utf-8') as f:\n",
    "    f.write(f\"qid\\tpid\\score\\n\")\n",
    "    for qid,pid_list in dev_qrels.items():\n",
    "        for pid in pid_list:\n",
    "            f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "with open('new_summary_origin_20000/qrels/test.tsv','w', encoding='utf-8') as f:\n",
    "    f.write(f\"qid\\tpid\\score\\n\")\n",
    "    for qid,pid_list in test_qrels.items():\n",
    "        for pid in pid_list:\n",
    "            f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "\n",
    "with open('new_summary_origin_20000/qrels/qrels.tsv','w', encoding='utf-8') as f:\n",
    "    f.write(f\"qid\\tpid\\score\\n\")\n",
    "    for qid,pid_list in summary_qrels.items():\n",
    "        for pid in pid_list:\n",
    "            f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformulate LLM Generated Example to queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('generated/generated_text_summary_corpus.json','r',encoding='utf-8') as f:\n",
    "    generated_query = json.load(f)#[0]\n",
    "    \n",
    "len(generated_query),generated_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_query[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in generated_query[:10]:\n",
    "    print(\"summary :\", pair['input'])\n",
    "    print(\"generated_text :\", pair['generated_text']['content'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "from tqdm import tqdm\n",
    "import re \n",
    "\n",
    "gen_all_q2id={}\n",
    "gen_query_dict = {}\n",
    "\n",
    "gen_summary_corpus_dict = {}\n",
    "gen_summary_qrels = defaultdict(list)\n",
    "\n",
    "\n",
    "# Post processing generated query / need to adapt \n",
    "\n",
    "for idx in tqdm(range(len(generated_query))):\n",
    "    split_lines = generated_query[idx]['generated_text']['content'].split('\\n')\n",
    "    for position, raw_query in enumerate(split_lines):\n",
    "        if ( 'generated_text' not in raw_query) and  ( 'summary:' not in raw_query) and ('키워드' not in raw_query) \\\n",
    "            and ('유의어 변경' not in raw_query) and ('# 출력' not in raw_query) and ('입력 문서:' not in raw_query) and ('입력문서:' not in raw_query):\n",
    "            splitted_query = raw_query.split(' ',1)\n",
    "            if len(splitted_query)<2 :\n",
    "                continue\n",
    "\n",
    "            if not re.findall('[-\\d.]\\s*',splitted_query[0]):\n",
    "                # print(raw_query)\n",
    "                continue\n",
    "            \n",
    "            query = splitted_query[1]\n",
    "            target = generated_query[idx]['input']\n",
    "\n",
    "        \n",
    "            if query in gen_all_q2id:\n",
    "                qid = gen_all_q2id[query]\n",
    "            else:\n",
    "                qid = f\"Q{len(gen_all_q2id)}\"\n",
    "                gen_all_q2id[query]=qid\n",
    "                gen_query_dict[qid]=query \n",
    "\n",
    "            pid = summary_p2id[target] # from summary_origin\n",
    "\n",
    "            gen_summary_qrels[qid].append(pid)\n",
    "\n",
    "\n",
    "len(gen_all_q2id),len(gen_query_dict),len(gen_summary_qrels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries\n",
    "import json \n",
    "import os \n",
    "\n",
    "os.makedirs('generated_summary_ver',exist_ok=True)\n",
    "\n",
    "with open('generated_summary_ver/queries.jsonl','w',encoding='utf-8') as f:\n",
    "    for k,pair in gen_query_dict.items():\n",
    "        f.write(json.dumps({'_id':k, 'text':pair},ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_summary_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict \n",
    "\n",
    "train_qrels = defaultdict(list)\n",
    "dev_qrels = defaultdict(list)\n",
    "test_qrels = defaultdict(list)\n",
    "\n",
    "# all_pairs = [(qid,pid) for qid, pid_list in gen_summary_qrels.items() for pid in pid_list]\n",
    "all_pairs = [(qid,pid) for qid, pid_list in gen_summary_qrels.items() for pid in pid_list]\n",
    "\n",
    "others, test = train_test_split(list(all_pairs), test_size=0.2) # 0.7 / 0.3\n",
    "train, dev = train_test_split(list(others), test_size=0.2) # 0.7 / 0.3\n",
    "\n",
    "print(len(all_pairs), len(train),len(dev),len(test)) # (12757, 8164, 2041, 2552)\n",
    "\n",
    "for pair in train: \n",
    "    qid, pid = pair\n",
    "    train_qrels[qid].append(pid)\n",
    "\n",
    "for pair in dev: \n",
    "    qid, pid = pair\n",
    "    dev_qrels[qid].append(pid)\n",
    "\n",
    "for pair in test: \n",
    "    qid, pid = pair\n",
    "    test_qrels[qid].append(pid)\n",
    "\n",
    "# for k,pairs in gen_summary_qrels.items():\n",
    "print(\"qrels:\", len(train_qrels), len(dev_qrels),len(test_qrels)) # qrels: 8162 2041 2552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_corpus[0]\n",
    "\n",
    "os.makedirs('generated_summary_ver/qrels',exist_ok=True)\n",
    "\n",
    "with open('generated_summary_ver/qrels/qrels.tsv','w', encoding='utf-8') as f:\n",
    "    f.write(f\"qid\\tpid\\score\\n\")\n",
    "    for qid,pid_list in gen_summary_qrels.items():\n",
    "        for pid in pid_list:\n",
    "            f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "\n",
    "###############\n",
    "#### Qrels for train, dev, test\n",
    "###############\n",
    "# with open('generated_summary_ver/qrels/qrels/train.tsv','w', encoding='utf-8') as f:\n",
    "#     f.write(f\"qid\\tpid\\score\\n\")\n",
    "#     for qid,pid_list in train_qrels.items():\n",
    "#         for pid in pid_list:\n",
    "#             f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "# with open('generated_summary_ver/qrels/dev.tsv','w', encoding='utf-8') as f:\n",
    "#     f.write(f\"qid\\tpid\\score\\n\")\n",
    "#     for qid,pid_list in dev_qrels.items():\n",
    "#         for pid in pid_list:\n",
    "#             f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "# with open('generated_summary_ver/qrels/test.tsv','w', encoding='utf-8') as f:\n",
    "#     f.write(f\"qid\\tpid\\score\\n\")\n",
    "#     for qid,pid_list in test_qrels.items():\n",
    "#         for pid in pid_list:\n",
    "#             f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n",
    "\n",
    "# with open('generated_summary_ver/qrels/qrels.tsv','w', encoding='utf-8') as f:\n",
    "#     f.write(f\"qid\\tpid\\score\\n\")\n",
    "#     for qid,pid_list in gen_summary_qrels.items():\n",
    "#         for pid in pid_list:\n",
    "#             f.write(f\"{qid}\\t{pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp new_summary_origin_20000/corpus.jsonl generated_summary_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformulate data for Contriever format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('generated_summary_ver/queries.jsonl','r',encoding='utf-8') as f:\n",
    "    query_summary_llm_gen = [json.loads(l) for l in f]\n",
    "    \n",
    "with open('new_summary_origin_20000/queries.jsonl','r',encoding='utf-8') as f:\n",
    "#     query_summary_origin = [json.loads(l) for l in f]\n",
    "    query_summary_origin = {json.loads(l)['text']:json.loads(l) for l in f}\n",
    "\n",
    "len(query_summary_llm_gen),len(query_summary_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary_llm_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('generated_summary_ver/corpus.jsonl','r',encoding='utf-8') as f:\n",
    "#     corpus_summary= [json.loads(l) for l in f]\n",
    "    corpus_summary= {json.loads(l)['_id']:json.loads(l) for l in f}\n",
    "    \n",
    "len(corpus_summary)#,corpus_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "qrels_summary_llm_gen=defaultdict(list)\n",
    "train_qrels_summary_llm_gen=defaultdict(list)\n",
    "dev_qrels_summary_llm_gen=defaultdict(list)\n",
    "test_qrels_summary_llm_gen=defaultdict(list)\n",
    "\n",
    "with open('generated_summary_ver/qrels/qrels.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        qrels_summary_llm_gen[qid].append(pid)\n",
    "\n",
    "tr_qrels_summary_origin=defaultdict(list)\n",
    "with open('new_summary_origin_20000/qrels/train.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        tr_qrels_summary_origin[qid].append(pid)\n",
    "\n",
    "len(tr_qrels_summary_origin)\n",
    "\n",
    "dev_qrels_summary_origin=defaultdict(list)\n",
    "with open('new_summary_origin_20000/qrels/dev.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        dev_qrels_summary_origin[qid].append(pid)\n",
    "\n",
    "len(dev_qrels_summary_origin)\n",
    "\n",
    "\n",
    "qrels_summary_llm_gen=defaultdict(list)\n",
    "train_qrels_summary_llm_gen=defaultdict(list)\n",
    "dev_qrels_summary_llm_gen=defaultdict(list)\n",
    "test_qrels_summary_llm_gen=defaultdict(list)\n",
    "\n",
    "\n",
    "with open('generated_summary_ver/qrels/qrels.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        qrels_summary_llm_gen[qid].append(pid)\n",
    "\n",
    "#################\n",
    "### Train, dev, test split\n",
    "#################\n",
    "with open('generated_summary_ver/qrels/train.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        train_qrels_summary_llm_gen[qid].append(pid)\n",
    "\n",
    "with open('generated_summary_ver/qrels/dev.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        dev_qrels_summary_llm_gen[qid].append(pid)\n",
    "\n",
    "with open('generated_summary_ver/qrels/test.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        test_qrels_summary_llm_gen[qid].append(pid)\n",
    "\n",
    "qrels_summary_origin=defaultdict(list)\n",
    "with open('new_summary_origin_20000/qrels/qrels.tsv','r') as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        qid,pid,score = l.strip().split('\\t')\n",
    "        qrels_summary_origin[qid].append(pid)\n",
    "\n",
    "len(train_qrels_summary_llm_gen),len(dev_qrels_summary_llm_gen),len(test_qrels_summary_llm_gen),len(qrels_summary_origin) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"question\":\"What is the most popular operating system?\",\"positive_ctxs\":[{\"text\": \"Windows is the most popular operating system.\"}],\"negative_ctxs\":[{\"text\": \"Windows is the most popular programming language.\"}],\"hard_negative_ctxs\":[{\"text\": \"Windows is the most popular game console.\"}],\"title\":\"Windows\",\"text\":\"Windows is the most popular operating system.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_for_contriever_finetune_summary_origin=[]\n",
    "dev_data_for_contriever_finetune_summary_origin=[]\n",
    "\n",
    "for k,v in tr_qrels_summary_origin.items():\n",
    "    query_info = query_summary_origin[int(k[1:])]\n",
    "    pos_info = [ {'title': corpus_summary[int(pid[1:])]['title'] , 'text': corpus_summary[int(pid[1:])]['text'] } for pid in v] \n",
    "\n",
    "    tr_data_for_contriever_finetune_summary_origin.append({\n",
    "        \"question\": query_info['text'],\n",
    "        \"positive_ctxs\": pos_info,\n",
    "        # \"negative_ctxs\": [],\n",
    "        # \"hard_negative_ctxs\": [] \n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "for k,v in dev_qrels_summary_origin.items():\n",
    "    query_info = query_summary_origin[int(k[1:])]\n",
    "    pos_info = [ {'title': corpus_summary[int(pid[1:])]['title'] , 'text': corpus_summary[int(pid[1:])]['text'] } for pid in v] \n",
    "\n",
    "    dev_data_for_contriever_finetune_summary_origin.append({\n",
    "        \"question\": query_info['text'],\n",
    "        \"positive_ctxs\": pos_info,\n",
    "        # \"negative_ctxs\": [],\n",
    "        # \"hard_negative_ctxs\": [] \n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "len(tr_data_for_contriever_finetune_summary_origin),len(dev_data_for_contriever_finetune_summary_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_summary_origin_20000/train.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "    for pair in tr_data_for_contriever_finetune_summary_origin:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open('new_summary_origin_20000/dev.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "    for pair in dev_data_for_contriever_finetune_summary_origin:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making data for new summary origin 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "    \n",
    "with open('new_summary_origin_20000/queries.jsonl','r',encoding='utf-8') as f:\n",
    "#     query_summary_origin = [json.loads(l) for l in f]\n",
    "    query_summary_origin = {json.loads(l)['text']:json.loads(l) for l in f}\n",
    "\n",
    "print(len(query_summary_llm_gen),len(query_summary_origin))\n",
    "\n",
    "with open('new_summary_origin_20000/corpus.jsonl','r',encoding='utf-8') as f:\n",
    "#     corpus_summary= [json.loads(l) for l in f]\n",
    "    corpus_summary= {json.loads(l)['_id']:json.loads(l) for l in f}\n",
    "    \n",
    "print(len(corpus_summary)) #,corpus_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('new_summary_origin_20000/train.w_negative.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "#     origin_contriever_data=[json.loads(l) for l in f]\n",
    "    origin_contriever_data={json.loads(l)['question']:json.loads(l) for l in f}\n",
    "\n",
    "len(origin_contriever_data)#,origin_contriever_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_summary_origin_20000/train.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "    tr_data_for_contriever_finetune_summary_origin = [json.loads(l) for l in f]\n",
    "        \n",
    "with open('new_summary_origin_20000/dev.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "    dev_data_for_contriever_finetune_summary_origin = [json.loads(l) for l in f]\n",
    "\n",
    "len(tr_data_for_contriever_finetune_summary_origin),len(dev_data_for_contriever_finetune_summary_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in tr_data_for_contriever_finetune_summary_origin:\n",
    "    query = pair['question']\n",
    "    info = origin_contriever_data[query]\n",
    "    pair['negative_ctxs']= info['negative_ctxs']\n",
    "\n",
    "for pair in dev_data_for_contriever_finetune_summary_origin:\n",
    "    query = pair['question']\n",
    "    info = origin_contriever_data[query]\n",
    "    pair['negative_ctxs']= info['negative_ctxs']\n",
    "\n",
    "len(tr_data_for_contriever_finetune_summary_origin),len(dev_data_for_contriever_finetune_summary_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_summary_origin_20000/train.w_negative.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "    for pair in tr_data_for_contriever_finetune_summary_origin:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open('new_summary_origin_20000/dev.w_negative.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "    for pair in dev_data_for_contriever_finetune_summary_origin:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard negative mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls processed/corpus2.size20000.summary_llm_gen.1012_ver1.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/test.filtered.data.for_contriever.jsonl','r') as f:\n",
    "    test= [json.loads(l) for l in f]\n",
    "len(test),test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    print(\"Question:\",i['question'])\n",
    "#     print(\"Target:\",i['positive_ctxs'][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "import random \n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import time\n",
    "# from easydict import EasyDict\n",
    "# from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "import glob\n",
    "import sys \n",
    "if not '/mnt/sda/hanseok/projects/nable_kaist/baselines/contriever' in sys.path:\n",
    "    sys.path.append('/mnt/sda/hanseok/projects/nable_kaist/baselines/contriever')\n",
    "    print(sys.path)\n",
    "import os \n",
    "\n",
    "# from contriever.src import contriever as contriever\n",
    "\n",
    "\n",
    "# Evaluator\n",
    "retriever = EvaluateRetrieval(None, score_function=None)\n",
    "metrics = defaultdict(list)  # store final results\n",
    "print(\"### Custom data mode\")\n",
    "# data_path = 'processed/corpus2.size20000.summary_llm_gen.1012_ver1.4'\n",
    "# data_path = 'processed/summary_origin2'\n",
    "# data_path = 'processed/corpus2.subset.summary_llm_gen.1021_ver1.5'\n",
    "data_path = 'processed/new_summary_origin_20000/'\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path,qrels_file = os.path.join(data_path,'qrels/train.tsv')).load_custom()\n",
    "# corpus, queries, qrels = GenericDataLoader(data_folder=data_path,qrels_file = os.path.join(data_path,'qrels/qrels.tsv')).load_custom()\n",
    "\n",
    "# BM25\n",
    "bm25_searcher = LuceneSearcher('../baselines/bm25/indexes/summary_origin2_ko')\n",
    "bm25_searcher.set_language('ko')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n",
    "reranker = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('processed/corpus2.subset.summary_llm_gen.1021_ver1.5/data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/train.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "with open('processed/summary_origin2/train.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "    contriever_data=[json.loads(l) for l in f]\n",
    "\n",
    "len(contriever_data),contriever_data[0].keys()\n",
    "\n",
    "# data_for_contriever_finetune_summary_origin.append({\n",
    "#         \"question\": query_info['text'],\n",
    "#         \"positive_ctxs\": pos_info,\n",
    "#         # \"negative_ctxs\": [],\n",
    "#         # \"hard_negative_ctxs\": [] \n",
    "#     })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in random.sample(contriever_data,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dataset filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "import random \n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "\n",
    "train_filtered_qrels = defaultdict(dict)\n",
    "rank_info=[]\n",
    "\n",
    "reranker.cuda()\n",
    "reranker.eval()\n",
    "\n",
    "cnt=0\n",
    "for idx,(qid, pid_dict) in tqdm(enumerate(qrels.items())): # train_qrels mode \n",
    "    query = queries[qid]\n",
    "    gold_pid = list(pid_dict.keys())[0]\n",
    "    p_info = corpus[gold_pid]\n",
    "    \n",
    "    reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512).to('cuda')\n",
    "    logits = reranker(**reranker_inputs).logits\n",
    "    true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "    if true_prob < 0.9: \n",
    "        # print(\"pass\")\n",
    "        cnt+=1\n",
    "        continue \n",
    "    \n",
    "    ######################\n",
    "    #### BM25 \n",
    "    ######################\n",
    "    hits = bm25_searcher.search(query)\n",
    "    \n",
    "    negative_passages = [] \n",
    "    flag=True \n",
    "    for i in range(len(hits)):\n",
    "        # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "        pred_docid = hits[i].docid\n",
    "        p_info = corpus[pred_docid]\n",
    "        \n",
    "        # if (pred_docid !=gold_pid):\n",
    "        #     flag=False\n",
    "\n",
    "        if (pred_docid !=gold_pid):# and (i==0):\n",
    "            rank_info.append(i+1)\n",
    "            flag=False \n",
    "            # reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512).to('cuda')\n",
    "            # logits = reranker(**reranker_inputs).logits\n",
    "            # true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "            # # if true_prob < 0.9:\n",
    "            # if true_prob <=1.0: # use all annotated negative\n",
    "            #     p_info.update({'ce_score':true_prob})\n",
    "#             negative_passages.append(p_info)\n",
    "            \n",
    "    # test_filtered_qrels[qid]={gold_pid:1}\n",
    "    if flag:\n",
    "#         filtered_contriever_data.append(contriever_data[idx])\n",
    "        train_filtered_qrels[qid]={gold_pid:1}\n",
    "        \n",
    "#     contriever_data[idx]['negative_ctxs']= negative_passages\n",
    "\n",
    "len(train_filtered_qrels), cnt, len(rank_info)\n",
    "# len(contriever_data), cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contriever_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_query_list = [queries[qid] for qid, pid_dict in train_filtered_qrels.items() if pid_dict]\n",
    "len(filtered_query_list),len(set(filtered_query_list))\n",
    "\n",
    "# filtered_test_contriever_data = []\n",
    "# for pair in test_contriever_data:\n",
    "#     if pair['question'] in filtered_query_list:\n",
    "#         filtered_test_contriever_data.append(pair)\n",
    "\n",
    "# len(filtered_test_contriever_data),filtered_test_contriever_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/train.w_negative.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "with open('processed/summary_origin2/train.w_negative.data.for_contriever.jsonl','w',encoding='utf-8') as f:\n",
    "    for l in contriever_data:\n",
    "        f.write(json.dumps(l,ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "retriever = EvaluateRetrieval(None, score_function=None)\n",
    "metrics = defaultdict(list)  # store final results\n",
    "print(\"### Custom data mode\")\n",
    "# data_path = 'processed/corpus2.size20000.summary_llm_gen.1012_ver1.4'\n",
    "# data_path = 'processed/corpus2.subset.summary_llm_gen.1021_ver1.5/'\n",
    "data_path = 'processed/summary_origin2'\n",
    "\n",
    "# corpus, queries, qrels = GenericDataLoader(data_folder=data_path,qrels_file = os.path.join(data_path,'qrels/test.tsv')).load_custom()\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path,qrels_file = os.path.join(data_path,'qrels/qrels.tsv')).load_custom()\n",
    "\n",
    "# BM25\n",
    "bm25_searcher = LuceneSearcher('../baselines/bm25/indexes/summary_origin2_ko')\n",
    "bm25_searcher.set_language('ko')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/test.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "with open('processed/summary_origin2/train.w_negative.data.for_contriever.jsonl','r',encoding='utf-8') as f:\n",
    "    test_contriever_data=[json.loads(l) for l in f]\n",
    "\n",
    "len(test_contriever_data),test_contriever_data[0].keys()\n",
    "\n",
    "# data_for_contriever_finetune_summary_origin.append({\n",
    "#         \"question\": query_info['text'],\n",
    "#         \"positive_ctxs\": pos_info,\n",
    "#         # \"negative_ctxs\": [],\n",
    "#         # \"hard_negative_ctxs\": [] \n",
    "#     })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "import random \n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "\n",
    "# for qid, pid_dict in random.sample(qrels.items(),10):\n",
    "\n",
    "test_filtered_qrels = defaultdict(list)\n",
    "high_lexical_test_filtered_qrels = defaultdict(list)\n",
    "\n",
    "reranker.cuda()\n",
    "reranker.eval()\n",
    "cnt=0\n",
    "rank_info=[]\n",
    "\n",
    "for idx,(qid, pid_dict) in tqdm(enumerate(qrels.items())): # train_qrels mode \n",
    "    query = queries[qid]\n",
    "    # pprint.pprint(f\"# Q_info: {qid} / {query}\")\n",
    "\n",
    "    gold_pid = list(pid_dict.keys())[0]\n",
    "    p_info = corpus[gold_pid]\n",
    "    # print(f\"# Gold Passage_info: {gold_pid} / \\n\",p_info)\n",
    "\n",
    "    reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512).to('cuda')\n",
    "    logits = reranker(**reranker_inputs).logits\n",
    "    true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "#     if true_prob < 0.9: \n",
    "#         cnt+=1\n",
    "#         continue \n",
    "    \n",
    "    ######################\n",
    "    #### BM25 \n",
    "    ######################\n",
    "    hits = bm25_searcher.search(query)\n",
    "    \n",
    "    # print(\"# Prediction - BM25\")\n",
    "    # negative_passages = [] \n",
    "    flag=True\n",
    "    for i in range(len(hits)):\n",
    "        # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "        pred_docid = hits[i].docid\n",
    "        p_info = corpus[pred_docid]\n",
    "        # pprint.pprint(f\"pred - {i}:\\n{p_info}\")\n",
    "\n",
    "        if (pred_docid ==gold_pid):\n",
    "            flag=False\n",
    "            rank_info.append(i+1)\n",
    "            \n",
    "            high_lexical_test_filtered_qrels[qid]={gold_pid:1}\n",
    "            break \n",
    "\n",
    "        if (pred_docid !=gold_pid) and (i==0):\n",
    "            reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512)\n",
    "            logits = reranker(**reranker_inputs).logits\n",
    "            true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "            if true_prob < 0.9:\n",
    "                negative_passages.append(p_info)\n",
    "\n",
    "        #         test_filtered_qrels[qid].append({gold_pid:1})\n",
    "                break\n",
    "        else:\n",
    "            break \n",
    "\n",
    "            # print(\"negative psgs:\",negative_passages)\n",
    "    contriever_data[idx]['hard_negative_ctxs']= negative_passages\n",
    "\n",
    "len(test_filtered_qrels),len(high_lexical_test_filtered_qrels), cnt, len(rank_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "import random \n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from tqdm import tqdm \n",
    "import torch \n",
    "\n",
    "# for qid, pid_dict in random.sample(qrels.items(),10):\n",
    "\n",
    "test_filtered_qrels = defaultdict(list)\n",
    "high_lexical_test_filtered_qrels = defaultdict(list)\n",
    "\n",
    "reranker.cuda()\n",
    "reranker.eval()\n",
    "cnt=0\n",
    "rank_info=[]\n",
    "\n",
    "for idx,(qid, pid_dict) in tqdm(enumerate(qrels.items())): # train_qrels mode \n",
    "    query = queries[qid]\n",
    "    # pprint.pprint(f\"# Q_info: {qid} / {query}\")\n",
    "\n",
    "    gold_pid = list(pid_dict.keys())[0]\n",
    "    p_info = corpus[gold_pid]\n",
    "    # print(f\"# Gold Passage_info: {gold_pid} / \\n\",p_info)\n",
    "\n",
    "    reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512).to('cuda')\n",
    "    logits = reranker(**reranker_inputs).logits\n",
    "    true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "    if true_prob < 0.9: \n",
    "#         print(\"pass\")\n",
    "        cnt+=1\n",
    "        continue \n",
    "    \n",
    "    ######################\n",
    "    #### BM25 \n",
    "    ######################\n",
    "    hits = bm25_searcher.search(query)\n",
    "    \n",
    "    # print(\"# Prediction - BM25\")\n",
    "    # negative_passages = [] \n",
    "    flag=True\n",
    "    for i in range(len(hits)):\n",
    "        # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "        pred_docid = hits[i].docid\n",
    "        p_info = corpus[pred_docid]\n",
    "        # pprint.pprint(f\"pred - {i}:\\n{p_info}\")\n",
    "\n",
    "        if (pred_docid ==gold_pid):\n",
    "            flag=False\n",
    "            rank_info.append(i+1)\n",
    "            \n",
    "            high_lexical_test_filtered_qrels[qid]={gold_pid:1}\n",
    "            break \n",
    "\n",
    "        # if (pred_docid !=gold_pid) and (i==0):\n",
    "        #     reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512)\n",
    "        #     logits = reranker(**reranker_inputs).logits\n",
    "        #     true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "        #     if true_prob < 0.9:\n",
    "        #         # negative_passages.append(p_info)\n",
    "\n",
    "        #         test_filtered_qrels[qid].append({gold_pid:1})\n",
    "        #         break\n",
    "        # else:\n",
    "        #     break \n",
    "\n",
    "            # print(\"negative psgs:\",negative_passages)\n",
    "    if flag:\n",
    "        test_filtered_qrels[qid]={gold_pid:1}\n",
    "\n",
    "    # contriever_data[idx]['hard_negative_ctxs']= negative_passages\n",
    "\n",
    "len(test_filtered_qrels),len(high_lexical_test_filtered_qrels), cnt, len(rank_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for qid, pid_dict in random.sample(qrels.items(),10):\n",
    "# for qid, pid_dict in qrels.items():\n",
    "#     if qid in test_filtered_qrels:\n",
    "    if not (qid in test_filtered_qrels):\n",
    "        query = queries[qid]\n",
    "        pprint.pprint(f\"# Q_info: {qid} / {query}\")\n",
    "    \n",
    "        gold_pid = list(pid_dict.keys())[0]\n",
    "        p_info = corpus[gold_pid]\n",
    "        print(f\"# Gold Passage_info: {gold_pid} \\n\",p_info)\n",
    "        hits = bm25_searcher.search(query,5)\n",
    "\n",
    "        print(\"# Prediction - BM25\")\n",
    "        for i in range(len(hits)):\n",
    "            print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "            pred_docid = hits[i].docid\n",
    "            p_info = corpus[pred_docid]\n",
    "            print(f\"pred - {i}:\\n{p_info}\")\n",
    "        print()\n",
    "#         input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "import pandas as pd \n",
    "pd.DataFrame(rank_info).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_dict= sorted(test_filtered_qrels.items(), key = lambda item: int(item[0][1:]),reverse=False)\n",
    "sorted_dict= sorted(high_lexical_test_filtered_qrels.items(), key = lambda item: int(item[0][1:]),reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sorted_dict),sorted_dict[1],len(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries['Q42'],corpus['C41']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../baselines/bm25/data/query/filtered.query_llm.query_llm.1012_ver1.4.tsv','w') as f:\n",
    "# with open('../baselines/bm25/data/query/high_lexical.filtered.query_llm.query_llm.1012_ver1.4.tsv','w') as f:\n",
    "with open('../baselines/bm25/data/query/high_lexical.filtered.summary_origin2.tsv','w') as f:\n",
    "# with open('../baselines/bm25/data/query/high_semantic.filtered.summary_origin2.tsv','w') as f:\n",
    "    for pair in sorted_dict:\n",
    "        k,v = pair\n",
    "        if not v:\n",
    "            continue\n",
    "        f.write(f\"{k}\\t{queries[k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/qrels/filtered.test.tsv','w') as f:\n",
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/qrels/high_lexical.filtered.test.tsv','w') as f:\n",
    "# with open('processed/summary_origin2/qrels/high_semantic.filtered.qrels.tsv','w') as f:\n",
    "with open('processed/summary_origin2/qrels/high_lexical.filtered.qrels.tsv','w') as f:\n",
    "    f.write(\"qid\\tpid\\tscore\\n\")\n",
    "    # for k,v in test_filtered_qrels.items():\n",
    "    for pair in sorted_dict:\n",
    "        k,v = pair\n",
    "        if not v:\n",
    "            continue\n",
    "        f.write(f\"{k}\\t{list(v.keys())[0]}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contriever_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_query_list = [queries[qid] for qid, pid_dict in test_filtered_qrels.items() if pid_dict]\n",
    "# len(filtered_query_list),len(set(filtered_query_list))\n",
    "\n",
    "high_lexical_filtered_query_list = [queries[qid] for qid, pid_dict in high_lexical_test_filtered_qrels.items() if pid_dict]\n",
    "len(high_lexical_filtered_query_list),len(set(high_lexical_filtered_query_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtered_test_contriever_data = []\n",
    "high_lexical_filtered_test_contriever_data = []\n",
    "for pair in test_contriever_data:\n",
    "#     if pair['question'] in filtered_query_list:\n",
    "    if pair['question'] in high_lexical_filtered_query_list:\n",
    "#         filtered_test_contriever_data.append(pair)\n",
    "        high_lexical_filtered_test_contriever_data.append(pair)\n",
    "\n",
    "# len(filtered_test_contriever_data),filtered_test_contriever_data[0]\n",
    "len(high_lexical_filtered_test_contriever_data),high_lexical_filtered_test_contriever_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/summary_origin2/lexical.filtered.data.for_contriever.jsonl','w') as f:\n",
    "# with open('processed/summary_origin2/semantic.filtered.data.for_contriever.jsonl','w') as f:\n",
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/test.filtered.data.for_contriever.jsonl','w') as f:\n",
    "# with open('processed/corpus2.size20000.summary_llm_gen.1012_ver1.4/test.high_lexical.filtered.data.for_contriever.jsonl','w') as f:\n",
    "#     for l in filtered_test_contriever_data:\n",
    "    for l in high_lexical_filtered_test_contriever_data:\n",
    "        f.write(json.dumps(l, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pid2qid = defaultdict(list)\n",
    "# for qid, pid_dict in test_filtered_qrels.items():\n",
    "for pair in sorted_dict:\n",
    "    qid,pid_dict = pair\n",
    "    if not pid_dict:\n",
    "        continue\n",
    "    query = queries[qid]\n",
    "    gold_pid = list(pid_dict.keys())[0]\n",
    "    pid2qid[gold_pid].append(qid)\n",
    "\n",
    "len(pid2qid), pd.DataFrame([len(v) for k,v in pid2qid.items()]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in pid2qid.items():\n",
    "for qid, pid_dict in test_filtered_qrels.items():\n",
    "# for pair in sorted_dict:\n",
    "    # qid,pid_dict = pair\n",
    "    \n",
    "    if not pid_dict:\n",
    "        continue\n",
    "\n",
    "    query = queries[qid]\n",
    "    pprint.pprint(f\"# Q_info: {qid} / {query}\")\n",
    "    \n",
    "    gold_pid = list(pid_dict.keys())[0]    \n",
    "    p_info = corpus[gold_pid]\n",
    "    pprint.pprint(f\"# Gold Passage_info: {p_info} \")\n",
    "    \n",
    "    # if len(v)>=5:\n",
    "    # print()\n",
    "    # for qid in v:\n",
    "    #     query = queries[qid]\n",
    "    #     pprint.pprint(f\"# Q_info: {qid} / {query}\")\n",
    "    \n",
    "    metrics = defaultdict(list)  # store final results\n",
    "    temp_result = defaultdict(dict)\n",
    "\n",
    "    hits = bm25_searcher.search(query)\n",
    "    \n",
    "    # print(\"# Prediction - BM25\")\n",
    "    for i in range(len(hits)):\n",
    "        # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
    "        pred_docid = hits[i].docid\n",
    "\n",
    "        temp_result[qid][pred_docid]=float(hits[i].score)\n",
    "        \n",
    "        p_info = corpus[pred_docid]\n",
    "        if i==0:\n",
    "            pprint.pprint(f\"pred - {i}:\\n{p_info}\")\n",
    "\n",
    "        # reranker_inputs= tokenizer(query,p_info['text'],return_tensors='pt',max_length=512)\n",
    "        # logits = reranker(**reranker_inputs).logits\n",
    "        # true_prob = torch.softmax(logits, dim=1).tolist()[0][1]\n",
    "        \n",
    "    # ndcg, _map, recall, precision = retriever.evaluate(qrels, temp_result, [1,10,100])#retriever.k_values)\n",
    "    ndcg, _map, recall, precision = retriever.evaluate(qrels, temp_result, [1,10])#retriever.k_values)\n",
    "    for metric in (ndcg, _map, recall, precision, \"mrr\", \"recall_cap\", \"hole\"):\n",
    "        if isinstance(metric, str):\n",
    "            metric = retriever.evaluate_custom(test_filtered_qrels, temp_result, retriever.k_values, metric=metric)\n",
    "        for key, value in metric.items():\n",
    "            metrics[key].append(value)\n",
    "\n",
    "    pprint.pprint(f\"NDCG@10:{metrics['NDCG@10']}\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c9f73431b21465cc74989402ac6c1fe3b8438058ec53b66285386964a794598"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
